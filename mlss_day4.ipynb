{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04d3b77a-5bf0-44ce-b8f9-c2931ed4a96e",
   "metadata": {},
   "source": [
    "## This notebook provides an illustrative demonstratation of how to finetune a pre-trained LLM model (DistillGpt2) on user-defined dataset and generate text using the finetuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa4c191-b824-4709-af27-de4181025499",
   "metadata": {},
   "source": [
    "## set env\n",
    "\n",
    "conda create -n mlss-day4\n",
    "\n",
    "conda activate mlss-day4\n",
    "\n",
    "conda install -c anaconda ipykernel\n",
    "\n",
    "python -m ipykernel install --user --name=mlss-day4\n",
    "\n",
    "pip install torch transformers datasets accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdc6227-659d-489d-9776-3e4db270f7b7",
   "metadata": {},
   "source": [
    "### Load Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fa15b8a-360b-414c-a93f-4afdb1d0b95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accelerate version: 1.7.0\n",
      "Transformers version: 4.52.3\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset, IterableDatasetDict\n",
    "import torch\n",
    "from itertools import islice\n",
    "from datasets import Dataset\n",
    "import accelerate\n",
    "import transformers\n",
    "print(\"Accelerate version:\", accelerate.__version__)\n",
    "print(\"Transformers version:\", transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757e6de7-3b16-43a0-b827-846942159bd9",
   "metadata": {},
   "source": [
    "### Load TinyStories dataset\n",
    "\n",
    "**Note: It does not load whole dataset in the memory, rather it gives a straming pointer to whole dataset, such that, during training we can load one token at a time, as loading whole dataset in the memory could consume huge amount of memory.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a531e347-53db-4b5f-92a7-e744b085ac9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28d37f26a61e4aef94c826527ff53fd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8894963f-c983-4815-9935-c15b3cb63f9d",
   "metadata": {},
   "source": [
    "### Load DistillGpt2 compatible tokenizer\n",
    "\n",
    "**Recall - In this notebook, we will explore finetuning and text generation using a pre-trained DistillGpt2 model.**\n",
    "\n",
    "1. This tokenizer converts raw text (e.g., 'Once upon a time') into token IDs understood by the model\n",
    "2. It includes special tokens, vocabulary size, and byte-pair encoding rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a5919de-ae85-4396-ae43-bfc8271c0358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d38be43549a43408a2c5f9f686f00f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d3247b8f0af41b7a4e52e72915a4c36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43d7c28b632f4f57b1edacd100eb7aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78bf40a6382d48bc969872b987e4a476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae3f4a4eb5d54930a45e5676a2bcb28d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")  # loads the pretrained tokenizer config and vocab for distilgpt2\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3082869-83f0-4483-b63b-95ea40781117",
   "metadata": {},
   "source": [
    "### Load pre-trained DistillGpt2 model in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "607ff563-6668-4287-a539-4440d7ebe703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "603e87de19694ff1aa0aa3fc13ad6d22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2af7ca8ced434a8ba3d23c71be9edd46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada2c777-28a4-4561-8a23-bda0e21e536b",
   "metadata": {},
   "source": [
    "### Tokenization Function\n",
    "\n",
    "Tokenize each input story and return special tokens mask for later masking if needed\n",
    "\n",
    "    - return_special_tokens_mask=True:\n",
    "    \n",
    "    - Adds a binary mask alongside token IDs\n",
    "    \n",
    "    - Value 0 for special tokens (e.g. , , <pad> if used)\n",
    "    \n",
    "    - Value 1 for normal tokens\n",
    "\n",
    "This is useful during training if we want to ignore loss on special tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ab98b77-db42-47b6-85ed-e911603b5c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "\n",
    "    return tokenizer(example[\"text\"], return_special_tokens_mask=True, truncation=True, max_length=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809e6adc-573c-45f8-93e0-7482336664a9",
   "metadata": {},
   "source": [
    "Reorganize token lists into fixed-length training chunks. Each final training sequence will be exactly 128 tokens long. \n",
    "\n",
    "It first flatten nested token lists for each key — e.g., [[1,2],[3]] → [1,2,3] and then split the flattened token list into chunks of 128 tokens each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "422e3b30-7b79-4e34-91c2-6115024d6dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):  \n",
    "    block_size = 128  \n",
    "    \n",
    "    concatenated = {k: sum((v if isinstance(v, list) else [v] for v in examples[k]), []) for k in examples}\n",
    "    total_length = len(concatenated[list(examples.keys())[0]])  \n",
    "    total_length = (total_length // block_size) * block_size  \n",
    "    \n",
    "    result = {\n",
    "        k: [t[i: i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated.items()\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039965d3-1449-4773-8aac-00c2066c8bd0",
   "metadata": {},
   "source": [
    "### Load only first 20000 samples from TinyStories dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f893d881-4518-4ac5-a365-cb4f22e9b24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_tokenized = dataset.map(tokenize_function)\n",
    "streaming_limited = islice(streaming_tokenized, 20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7c06ae-ddb0-419d-ba47-2b7f634bbd4e",
   "metadata": {},
   "source": [
    "### Convert to a dataset-like object compatible with Trainer, and load the tokens in the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "feb9793d-0b57-430a-a5e4-c4e676f12762",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = list(streaming_limited)\n",
    "batched = [buffer[i:i + 1000] for i in range(0, len(buffer), 1000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1746dff7-acca-442e-ac34-a92901bf4e8b",
   "metadata": {},
   "source": [
    "### Group all the tokens as 128 tokens long sequence for finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fdd6c9b-57c8-43e6-952b-1f9cb6ce9c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = []\n",
    "\n",
    "for batch in batched:\n",
    "    \n",
    "    batch_dict = {k: [d[k] for d in batch] for k in batch[0].keys()}\n",
    "    grouped_chunk = group_texts(batch_dict)\n",
    "    \n",
    "    grouped.extend([{k: v[i] for k, v in grouped_chunk.items()} for i in range(len(grouped_chunk[list(grouped_chunk.keys())[0]]))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a15d77-c2cb-45f8-9dba-ffb13158fa04",
   "metadata": {},
   "source": [
    "### Make dataset compatible with the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6cb6a66-c7b8-4049-ae9f-e33fca267f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_list(grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22f09542-6cbf-43c8-af48-6061010823c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96d7f688-833f-4138-bd59-5df8bba9b2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'input_ids', 'attention_mask', 'special_tokens_mask'],\n",
      "    num_rows: 140\n",
      "})\n",
      "140\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b36adef-56cc-4e70-8eb3-11f9452e5921",
   "metadata": {},
   "source": [
    "### Visualize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5eca4ee-e625-4024-8166-28fc5e7cef33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  [One day, a little girl named Lily found a nee...   \n",
      "1  [Once upon a time there was a very playful pup...   \n",
      "2  [Once upon a time there were two best friends ...   \n",
      "3  [The little boy lived in a quiet house. He lov...   \n",
      "4  [Once upon a time, there was a weird rocket. T...   \n",
      "5  [Once upon a time, there was a strong cone in ...   \n",
      "6  [Sara and Tom are twins. They like to visit th...   \n",
      "7  [Once upon a time, there was a little girl nam...   \n",
      "8  [Danny wanted some juice, but he didn't have a...   \n",
      "9  [Once, there was a farmer who had to make a lo...   \n",
      "\n",
      "                                           input_ids  \\\n",
      "0  [3198, 1110, 11, 257, 1310, 2576, 3706, 20037,...   \n",
      "1  [290, 5742, 1123, 584, 13, 2293, 484, 5201, 11...   \n",
      "2  [262, 5509, 290, 7342, 262, 5667, 2121, 319, 6...   \n",
      "3  [3114, 379, 4463, 290, 531, 11, 366, 2949, 11,...   \n",
      "4  [345, 11, 1310, 5916, 11, 329, 1642, 502, 1254...   \n",
      "5  [23612, 5509, 407, 284, 307, 6507, 13, 383, 23...   \n",
      "6  [5615, 287, 257, 1263, 16669, 351, 607, 1266, ...   \n",
      "7  [7454, 2402, 257, 640, 11, 612, 373, 257, 1310...   \n",
      "8  [7624, 810, 20037, 550, 1364, 683, 13, 1375, 3...   \n",
      "9  [13, 2293, 257, 981, 11, 26154, 11638, 503, 26...   \n",
      "\n",
      "                                      attention_mask  \\\n",
      "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "5  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "6  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "7  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "8  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "9  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "\n",
      "                                 special_tokens_mask  \n",
      "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "5  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "6  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "7  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "8  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "9  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "count    140.0\n",
      "mean     128.0\n",
      "std        0.0\n",
      "min      128.0\n",
      "25%      128.0\n",
      "50%      128.0\n",
      "75%      128.0\n",
      "max      128.0\n",
      "Name: input_ids, dtype: float64\n",
      "count    140.0\n",
      "mean     128.0\n",
      "std        0.0\n",
      "min      128.0\n",
      "25%      128.0\n",
      "50%      128.0\n",
      "75%      128.0\n",
      "max      128.0\n",
      "Name: attention_mask, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "# Use Hugging Face's built-in conversion\n",
    "df = train_dataset.to_pandas()\n",
    "\n",
    "# Inspect the structure\n",
    "print(df.head(10))\n",
    "\n",
    "# Analyze token sequence lengths\n",
    "print(df['input_ids'].apply(len).describe())\n",
    "print(df['attention_mask'].apply(len).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142c424c-54cb-4e80-9b26-cba5559137da",
   "metadata": {},
   "source": [
    "### Define DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4aadbd0d-efd9-48c1-9fea-e03fa99d1f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf47369-0335-4d9b-b5ef-d43940e906de",
   "metadata": {},
   "source": [
    "### Define Training Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ed6e076-3112-4ce5-9fe8-807b14be6aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tinystories_output\",        # directory to save model checkpoints\n",
    "    overwrite_output_dir=True,                # overwrite old output if exists\n",
    "    num_train_epochs=10,                      # number of training epochs\n",
    "    per_device_train_batch_size=2,            # batch size per GPU\n",
    "    save_steps=500,                           # save model every 500 steps\n",
    "    save_total_limit=1,                       # only keep last checkpoint\n",
    "    logging_steps=100,                        # log every 100 steps\n",
    "    prediction_loss_only=True,                # don't store predictions, just loss\n",
    "    fp16=torch.cuda.is_available()            # use mixed precision if GPU supports it\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06f256b-cc27-46ff-aeee-cfb8ff2f8399",
   "metadata": {},
   "source": [
    "### Define Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da3447f9-2a6d-4bac-a64b-0084ab721c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                       # the language model to train\n",
    "    args=training_args,                # training hyperparameters\n",
    "    data_collator=data_collator,       # handles dynamic padding during training\n",
    "    train_dataset=train_dataset        # our processed and limited TinyStories dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891dedba-34d3-4051-8e41-76ca2f05387f",
   "metadata": {},
   "source": [
    "### Finetune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2aea029b-cb0b-4ad7-b0ab-536c31bc1c90",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [700/700 25:03, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.741300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.287200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.031200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.866200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.714700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.634000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.569000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=700, training_loss=1.9776550946916853, metrics={'train_runtime': 1508.7765, 'train_samples_per_second': 0.928, 'train_steps_per_second': 0.464, 'total_flos': 45726931353600.0, 'train_loss': 1.9776550946916853, 'epoch': 10.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a685ae4-16dc-4bc9-9639-bd38a296105c",
   "metadata": {},
   "source": [
    "### Define prompt for text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4c55184-bb64-49a0-bdc0-7d2709e2c9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Once upon a time\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77a085b-b3a5-4c8b-adf1-dfa420ef965c",
   "metadata": {},
   "source": [
    "### Tokenize the given prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfbb2673-18bc-4b76-abae-e64fe64a04b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the prompt into model input\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "# Move tensors to the correct device (CPU/GPU)\n",
    "inputs = {key: val.to(model.device) for key, val in inputs.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f99d9c0-ca5b-4845-9176-c2a6132178be",
   "metadata": {},
   "source": [
    "### Generate text using the given prompt of max-length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f4a0a25-fa0b-4fa1-be8f-268f1bb1deef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was a little boy named Tim. Tim loved to play outside and enjoy the outdoors. One day, he decided to go out and play in his yard. Tim saw lots of colorful things floating around. One day, his mom gave him some food and ran over.\n",
      "\n",
      "Tim and his mom helped him get some food for himself. They all had so much to say about, but they all needed to share some food for the trip. Tim thought about sharing something special\n"
     ]
    }
   ],
   "source": [
    "# Generate continuation with sampling\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_length=100,           # max number of tokens in generated sequence\n",
    "    do_sample=True,           # sample instead of greedy decoding\n",
    "    temperature=0.8           # controls randomness (lower = more deterministic)\n",
    ")\n",
    "\n",
    "# Decode tokens into readable text, skip special tokens like  \n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df4f4f1-d0bf-48e3-af22-4c5d625d1969",
   "metadata": {},
   "source": [
    "### Generate text using the given prompt of max-length = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02b7a19f-c30d-488d-bca6-42193941be8f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there lived a quiet, healthy little girl. She always wanted to have fun, but she also wanted to keep the fun around her.\n",
      "\n",
      "One day, while walking, she saw a big tree and wanted to see how it grew. She wanted to see if it could grow, but she was too busy spinning it around.\n",
      "\n",
      "Suddenly, the tree was too tall and fell under her, but she knew it would take her to get there. She tried to make sure it kept her feet up, but the tree made her angry.\n",
      "\n",
      "Her mom told her that it was best to stay up close to the tree, but she had to stay safe. She decided to stay up close to the tree too. She carefully covered the tree and closed the tree.\n",
      "\n",
      "The tree went into a state of being in the shade when it finally started to grow. The rain made the tree wet and wet. When it finally came to a stop, the tree felt warm and strong. It felt like it had started to grow. The tree started to grow more and more trees started to grow.\n",
      "\n",
      "The tree was so happy that it had grown, that it had been able to stay warm and stay in the\n"
     ]
    }
   ],
   "source": [
    "# Generate continuation with sampling\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_length=250,           # max number of tokens in generated sequence\n",
    "    do_sample=True,           # sample instead of greedy decoding\n",
    "    temperature=0.8           # controls randomness (lower = more deterministic)\n",
    ")\n",
    "\n",
    "# Decode tokens into readable text, skip special tokens like  \n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f52481-86a8-4204-8f83-5430cbafae16",
   "metadata": {},
   "source": [
    "### Generate text using the given prompt of max-length = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ae01107-6d6f-4174-97e2-641d2ef10279",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was a little girl named Lily. She liked to cook with her friends, especially on her favorite meal. One day, she came back to visit her mom. She was so happy, and she had a delicious plate made out of crayons.\n",
      "\n",
      "Lily was so excited! She made a big cake with crayons that she'd eat all day. She ate it with her family, friends, and the car. They celebrated the cake and took the cake. They were so happy. And then, they celebrated the cake and walked over to the kitchen with their hands clasped together. They were so happy and happy.\n",
      "\n",
      "From that day on, Lily made a few crayons for her family. It was so warm and inviting that they made a big cake with their hands and crayons on their laps. They were so happy! They were so happy! They watched them and they said goodbye.\n",
      "\n",
      "Lily was so happy that she decided to share her cake and make a big cake with her friends. The cake was so happy! She made a big cake and shared it with everyone. \n",
      "The next day, Lily made a big cake with her mom and dad. They were so glad they shared the cake. They hugged and hugged and laughed. \n",
      "Lily's Mommy and Dad hugged her and they shared the cake and ate it with her mom and dad. \n",
      "Lily made a big cake and shared it with her dad and Mom. They hugged and cried together, and hugged and hugged. \n",
      "But the fun started for the little girl. They ate her cake and made a big cake. They played outside and had fun. One sunny day, they ate their cake and enjoyed the sunshine. They hugged and hugged and hugged. They both ate their cake and enjoyed their cake and enjoyed the sunshine.\n",
      "\n",
      "Lily smiled and smiled and hugged her mom for the happy day. She smiled, smiled and thanked the cake and hugged her dad for sharing the cake with her and dad. They both enjoyed the cake and enjoyed the sunshine. \n",
      "Lily had some fun too. She played with her mom and dad and played with them all day. They laughed and laughed, because they were friends. \n",
      "Later that day, Lily and her dad drove her to their house. They stayed in the car for a long time. They drove around happily ever after, and they all drove around the\n"
     ]
    }
   ],
   "source": [
    "# Generate continuation with sampling\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_length=500,           # max number of tokens in generated sequence\n",
    "    do_sample=True,           # sample instead of greedy decoding\n",
    "    temperature=0.8           # controls randomness (lower = more deterministic)\n",
    ")\n",
    "\n",
    "# Decode tokens into readable text, skip special tokens like  \n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e160f88-5a66-443d-adbf-b9e85c9aeb75",
   "metadata": {},
   "source": [
    "### Generate text using the given prompt of max-length = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80f51b0a-baf4-4105-a1da-1ada948b7c29",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was a little girl named Lucy. She liked to play with her toys and toys. But a few years ago, Lucy's mom gave her permission. Lucy didn't like them, so she decided to put them in a tree. Lucy thought she had to make a big tree with a branch. She decided to make a small tree with a branch with her key. She made two plants with a needle. One day, she saw a big monster in the forest. He was so fast! \n",
      "\n",
      "\n",
      "Suddenly, the monster looked up at Lucy and said, \"You're the monster. You are the little girl who loves to play alone. Do you have a soft tongue?\" Lucy's mom asked. Lucy shook her head.\n",
      "\n",
      "\"No, I don't think so,\" she said.\n",
      "\n",
      "\"Let's just pretend you are a monster and pretend you are a family member.\"\n",
      "\n",
      "Lucy's mom laughed and said, \"Did you know that you are a little girl now?\"\n",
      "\n",
      "And so, Lucy walked away. She loved to play with her toys. She loved to play with her friends and share toys so she could support them.\n",
      "One day, Lucy went to the library to see what she could learn. She saw a beautiful elephant with a picture on it. She said, \"One day, I'll design a rocket that can fly over the land.\" \n",
      "\n",
      "Lucy listened to her mom and thought, \"Okay, let's take a rocket! The rocket is made from scratch.\" She went to the rocket and saw a little girl who was very happy. The rocket flew away and laughed. When it landed, the little girl was surprised and happy. She said, \"Thank you, Lucy. You have allowed me to design a rocket that can fly over the land!\" \n",
      "\n",
      "Lucy's mom smiled and said, \"This rocket is perfect! It is a rocket that can fly over the land!\" \n",
      "\n",
      "When she decided to design a rocket, her mom laughed and said, \"It can fly over the land!\" \n",
      "\n",
      "Lucy was so excited and surprised. She thought, \"Let's try something new!\" \n",
      "\n",
      "Her mom said, \"Let's try something new!\" \n",
      "\n",
      "Lucy's mom said, \"Let's look at what we can make together.\" \n",
      "Lucy smiled and smiled. She didn't have to learn to draw draw a rocket. She had a plan to draw a rocket. She asked her mom, \"What should I draw?\" \n",
      "Her mom smiled and said, \"Let's make our rocket and give it something new shape.\" \n",
      "\n",
      "Lucy's mom smiled and said, \"Let's see what we can make with that rocket!\" \n",
      "\n",
      "Her mom smiled and said, \"My rocket is very useful in our day.\" \n",
      "\n",
      "Lucy's mom smiled and hugged her. She wanted to help her family and learn more. They went to the library to see what they could do with their rocket. They were amazed. They had a rocket with lots of parts and lots of colors. They wanted to design a rocket that could fly over the land. They thought, \"Let's make a rocket!\" \n",
      "\n",
      "But Lucy didn't hear her mom's permission and she said, \"Here, I put a rocket on my rocket!\" \n",
      "\n",
      "Her mom smiled and said, \"Let's make a rocket!\" \n",
      "\n",
      "Lucy's mom smiled and said, \"Let's try something new!\" \n",
      "Her mom smiled and said, \"Let's make a rocket!\" \n",
      "\n",
      "Lucy's mom smiled and said, \"Let's make a rocket!\" \n",
      "\n",
      "Her mom smiled and said, \"Let's make a rocket!\" \n",
      "\n",
      "Her mom smiled and said, \"Let's make a rocket!\" \n",
      "\n",
      "Lucy's mom smiled and smiled and said, \"Let's make a rocket!\" \n",
      "\n",
      "Her mom smiled and said, \"Let's make a rocket!\" \n",
      "\n",
      "Her mom smiled and said, \"Be careful with your rocket! It can make lots of things! It is very difficult to make a rocket! The rocket can make many things! It can make many things! It can make many things! It can make lots of things.\" \n",
      "\n",
      "The little girl thought she was going to make a rocket. She said, \"My rocket is very special to me. It can make many things!\" \n",
      "\n",
      "The little girl shook her head and said, \"I love my rocket!\" \n",
      "\n",
      "Her mom smiled and said, \"Just give me something else. I will make a rocket!\" \n",
      "\n",
      "\n",
      "The little girl smiled and said, \"I will make something special! I will make a rocket!\" \n",
      "\n",
      "She was so happy and happy that she made a rocket. She loved to play with her friends\n"
     ]
    }
   ],
   "source": [
    "# Generate continuation with sampling\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_length=1000,           # max number of tokens in generated sequence\n",
    "    do_sample=True,           # sample instead of greedy decoding\n",
    "    temperature=0.8           # controls randomness (lower = more deterministic)\n",
    ")\n",
    "\n",
    "# Decode tokens into readable text, skip special tokens like  \n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
