{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c5d72f4",
   "metadata": {},
   "source": [
    "# Finetune with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fd9cda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers datasets lightning watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "033b75c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/a14-hliu/miniconda3/envs/py311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch       : 2.5.1\n",
      "transformers: 4.46.2\n",
      "datasets    : 3.1.0\n",
      "lightning   : 2.5.1.post0\n",
      "\n",
      "conda environment: py311\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark --conda -p torch,transformers,datasets,lightning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59e5aefc-40b8-4361-bc2a-6e12d25235d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1 Loading the dataset into DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39e2228-5f0b-4fb9-b762-df26c2052b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Custom utilities for downloading and preparing the dataset\n",
    "from local_dataset_utilities import download_dataset, load_dataset_into_to_dataframe, partition_dataset\n",
    "from local_dataset_utilities import IMDBDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed9af91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure GPU is available before proceeding\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"Please switch to a GPU machine before running this notebook.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb31ac90-9e3a-41d0-baf1-8e613043924b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine whether to download dataset or not based on existence of files\n",
    "\n",
    "files = (\"test.csv\", \"train.csv\", \"val.csv\")\n",
    "download = True\n",
    "\n",
    "for f in files:\n",
    "    if not os.path.exists(os.path.join(\"data\", f)):\n",
    "        download = False\n",
    "\n",
    "if download is False:\n",
    "    download_dataset()\n",
    "    df = load_dataset_into_to_dataframe()\n",
    "    partition_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221f30a1-b433-4304-a18d-8d03abd42b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load already-prepared train/val/test CSVs\n",
    "\n",
    "df_train = pd.read_csv(os.path.join(\"data\", \"train.csv\"))\n",
    "df_val = pd.read_csv(os.path.join(\"data\", \"val.csv\"))\n",
    "df_test = pd.read_csv(os.path.join(\"data\", \"test.csv\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "846d83b1",
   "metadata": {},
   "source": [
    "# 2 Tokenization and Numericalization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2bd5f770",
   "metadata": {},
   "source": [
    "**Load the dataset via `load_dataset`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1aa66c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['index', 'text', 'label'],\n",
      "        num_rows: 19\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['index', 'text', 'label'],\n",
      "        num_rows: 14\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['index', 'text', 'label'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load the CSVs into Hugging Face's DatasetDict format\n",
    "\n",
    "imdb_dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "        \"train\": os.path.join(\"data\", \"train.csv\"),\n",
    "        \"validation\": os.path.join(\"data\", \"val.csv\"),\n",
    "        \"test\": os.path.join(\"data\", \"test.csv\"),\n",
    "    },\n",
    ")\n",
    "\n",
    "print(imdb_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "029adc8f-cdfe-4386-9552-a1120f49adee",
   "metadata": {},
   "source": [
    "**Tokenize the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea762ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer input max length: 512\n",
      "Tokenizer vocabulary size: 30522\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer from Hugging Face for distilBERT\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "print(\"Tokenizer input max length:\", tokenizer.model_max_length)\n",
    "print(\"Tokenizer vocabulary size:\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8432c15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset for BERT-style input \n",
    "\n",
    "def tokenize_text(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb392cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenizer to all splits\n",
    "\n",
    "imdb_tokenized = imdb_dataset.map(tokenize_text, batched=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4103c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory by deleting original dataset\n",
    "del imdb_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ef894c-978f-47f2-9d61-cb6a9f38e745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set tensor format for PyTorch and specify usable columns\n",
    "imdb_tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea67091-aeb7-46c1-871f-638ce58d8a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable parallel tokenizer warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c4f8cd8-e641-45fb-9893-70677631917a",
   "metadata": {},
   "source": [
    "# 3 Set Up DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0807b068-7d8f-4055-a26a-177e07dea4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom PyTorch Dataset wrapper for DataLoader\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, dataset_dict, partition_key=\"train\"):\n",
    "        self.partition = dataset_dict[partition_key]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.partition[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.partition.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cb08f3-ef77-4351-8b19-42d99dd24f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets for each split\n",
    "train_dataset = IMDBDataset(imdb_tokenized, partition_key=\"train\")\n",
    "val_dataset = IMDBDataset(imdb_tokenized, partition_key=\"validation\")\n",
    "test_dataset = IMDBDataset(imdb_tokenized, partition_key=\"test\")\n",
    "\n",
    "# Create dataloaders for training, validation, and testing\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=12,\n",
    "    shuffle=True, \n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=12,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=12,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c977b07-2559-4ff7-85c4-d62785e81558",
   "metadata": {},
   "source": [
    "# 4 Initializing DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b1e449-7e00-4965-9883-f97374503996",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model for sequence classification\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9854e27-fc4f-418e-996f-a8ef04a295ac",
   "metadata": {},
   "source": [
    "**Freeze all layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14e61f3-3279-4725-ad7a-3c77f85aa39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all parameters initially\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6d3e532-3684-4dbf-a8ce-60b6c6675a00",
   "metadata": {},
   "source": [
    "**Add LoRA layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bca22b5-8a48-4006-929f-66734a745a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model # show model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296f865e-68c6-4bc8-b352-350f3d7f7a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define LoRA (Low-Rank Adaptation) Modules ---\n",
    "# LoRA introduces trainable low-rank matrices into frozen pretrained models to reduce memory usage and training cost.\n",
    "\n",
    "\n",
    "class LoRALayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        \"\"\"\n",
    "        Implements a low-rank decomposition of a linear transformation.\n",
    "        Instead of training a full weight matrix, LoRA trains two smaller matrices:\n",
    "        W_a (in_dim x rank) and W_b (rank x out_dim), where rank << in_dim, out_dim.\n",
    "        alpha is a scaling factor to control the contribution of LoRA.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        # W_a is initialized randomly with a small standard deviation\n",
    "        self.W_a = torch.nn.Parameter(torch.randn(in_dim, rank) * std_dev)\n",
    "        # W_b is initialized to zero\n",
    "        self.W_b = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Applies the low-rank adaptation and scales it by alpha\n",
    "        x = self.alpha * (x @ self.W_a @ self.W_b)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LinearWithLoRA(torch.nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        \"\"\"\n",
    "        Wraps a standard linear layer with an additive LoRA module.\n",
    "        The original linear layer remains frozen, and only LoRA parameters are trainable.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = linear # Frozen linear layer from pre-trained model\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Output = original frozen linear output + LoRA adjustment\n",
    "        return self.linear(x) + self.lora(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d73e454-f0d8-411c-ba49-7cd46cf0bf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configure LoRA injection ---\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "# LoRA configuration\n",
    "lora_r = 8 # Rank of the low-rank matrices (controls trainable parameter size)\n",
    "lora_alpha = 16 # Scaling factor for LoRA output\n",
    "lora_dropout = 0.05\n",
    "\n",
    "# Flags to control which parts of the model receive LoRA injections\n",
    "lora_query = True # Apply LoRA to query projection in self-attention\n",
    "lora_key = False # Apply LoRA to key projection\n",
    "lora_value = True # Apply LoRA to value projection\n",
    "lora_projection = False # Apply LoRA to attention output projection\n",
    "lora_mlp = False # Apply LoRA to feed-forward MLP layers\n",
    "lora_head = False # Apply LoRA to final classifier head\n",
    "\n",
    "layers = []\n",
    "\n",
    "# Create a partial function to simplify LoRA module creation\n",
    "assign_lora = partial(LinearWithLoRA, rank=lora_r, alpha=lora_alpha)\n",
    "\n",
    "# Inject LoRA into specified layers of each Transformer block in DistilBERT\n",
    "for layer in model.distilbert.transformer.layer:\n",
    "    if lora_query:\n",
    "        layer.attention.q_lin = assign_lora(layer.attention.q_lin)\n",
    "    if lora_key:\n",
    "        layer.attention.k_lin = assign_lora(layer.attention.k_lin)\n",
    "    if lora_value:\n",
    "        layer.attention.v_lin = assign_lora(layer.attention.v_lin)\n",
    "    if lora_projection:\n",
    "        layer.attention.out_lin = assign_lora(layer.attention.out_lin)\n",
    "    if lora_mlp:\n",
    "        layer.ffn.lin1 = assign_lora(layer.ffn.lin1)\n",
    "        layer.ffn.lin2 = assign_lora(layer.ffn.lin2)\n",
    "if lora_head:\n",
    "    model.pre_classifier = assign_lora(model.pre_classifier)\n",
    "    model.classifier = assign_lora(model.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9794ef2-e074-4e13-af17-990a499b03bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): LinearWithLoRA(\n",
       "              (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): LinearWithLoRA(\n",
       "              (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f852fc6-3b00-4831-a973-e03b539a6cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert.embeddings.word_embeddings.weight: False\n",
      "distilbert.embeddings.position_embeddings.weight: False\n",
      "distilbert.embeddings.LayerNorm.weight: False\n",
      "distilbert.embeddings.LayerNorm.bias: False\n",
      "distilbert.transformer.layer.0.attention.q_lin.linear.weight: False\n",
      "distilbert.transformer.layer.0.attention.q_lin.linear.bias: False\n",
      "distilbert.transformer.layer.0.attention.q_lin.lora.W_a: True\n",
      "distilbert.transformer.layer.0.attention.q_lin.lora.W_b: True\n",
      "distilbert.transformer.layer.0.attention.k_lin.weight: False\n",
      "distilbert.transformer.layer.0.attention.k_lin.bias: False\n",
      "distilbert.transformer.layer.0.attention.v_lin.linear.weight: False\n",
      "distilbert.transformer.layer.0.attention.v_lin.linear.bias: False\n",
      "distilbert.transformer.layer.0.attention.v_lin.lora.W_a: True\n",
      "distilbert.transformer.layer.0.attention.v_lin.lora.W_b: True\n",
      "distilbert.transformer.layer.0.attention.out_lin.weight: False\n",
      "distilbert.transformer.layer.0.attention.out_lin.bias: False\n",
      "distilbert.transformer.layer.0.sa_layer_norm.weight: False\n",
      "distilbert.transformer.layer.0.sa_layer_norm.bias: False\n",
      "distilbert.transformer.layer.0.ffn.lin1.weight: False\n",
      "distilbert.transformer.layer.0.ffn.lin1.bias: False\n",
      "distilbert.transformer.layer.0.ffn.lin2.weight: False\n",
      "distilbert.transformer.layer.0.ffn.lin2.bias: False\n",
      "distilbert.transformer.layer.0.output_layer_norm.weight: False\n",
      "distilbert.transformer.layer.0.output_layer_norm.bias: False\n",
      "distilbert.transformer.layer.1.attention.q_lin.linear.weight: False\n",
      "distilbert.transformer.layer.1.attention.q_lin.linear.bias: False\n",
      "distilbert.transformer.layer.1.attention.q_lin.lora.W_a: True\n",
      "distilbert.transformer.layer.1.attention.q_lin.lora.W_b: True\n",
      "distilbert.transformer.layer.1.attention.k_lin.weight: False\n",
      "distilbert.transformer.layer.1.attention.k_lin.bias: False\n",
      "distilbert.transformer.layer.1.attention.v_lin.linear.weight: False\n",
      "distilbert.transformer.layer.1.attention.v_lin.linear.bias: False\n",
      "distilbert.transformer.layer.1.attention.v_lin.lora.W_a: True\n",
      "distilbert.transformer.layer.1.attention.v_lin.lora.W_b: True\n",
      "distilbert.transformer.layer.1.attention.out_lin.weight: False\n",
      "distilbert.transformer.layer.1.attention.out_lin.bias: False\n",
      "distilbert.transformer.layer.1.sa_layer_norm.weight: False\n",
      "distilbert.transformer.layer.1.sa_layer_norm.bias: False\n",
      "distilbert.transformer.layer.1.ffn.lin1.weight: False\n",
      "distilbert.transformer.layer.1.ffn.lin1.bias: False\n",
      "distilbert.transformer.layer.1.ffn.lin2.weight: False\n",
      "distilbert.transformer.layer.1.ffn.lin2.bias: False\n",
      "distilbert.transformer.layer.1.output_layer_norm.weight: False\n",
      "distilbert.transformer.layer.1.output_layer_norm.bias: False\n",
      "distilbert.transformer.layer.2.attention.q_lin.linear.weight: False\n",
      "distilbert.transformer.layer.2.attention.q_lin.linear.bias: False\n",
      "distilbert.transformer.layer.2.attention.q_lin.lora.W_a: True\n",
      "distilbert.transformer.layer.2.attention.q_lin.lora.W_b: True\n",
      "distilbert.transformer.layer.2.attention.k_lin.weight: False\n",
      "distilbert.transformer.layer.2.attention.k_lin.bias: False\n",
      "distilbert.transformer.layer.2.attention.v_lin.linear.weight: False\n",
      "distilbert.transformer.layer.2.attention.v_lin.linear.bias: False\n",
      "distilbert.transformer.layer.2.attention.v_lin.lora.W_a: True\n",
      "distilbert.transformer.layer.2.attention.v_lin.lora.W_b: True\n",
      "distilbert.transformer.layer.2.attention.out_lin.weight: False\n",
      "distilbert.transformer.layer.2.attention.out_lin.bias: False\n",
      "distilbert.transformer.layer.2.sa_layer_norm.weight: False\n",
      "distilbert.transformer.layer.2.sa_layer_norm.bias: False\n",
      "distilbert.transformer.layer.2.ffn.lin1.weight: False\n",
      "distilbert.transformer.layer.2.ffn.lin1.bias: False\n",
      "distilbert.transformer.layer.2.ffn.lin2.weight: False\n",
      "distilbert.transformer.layer.2.ffn.lin2.bias: False\n",
      "distilbert.transformer.layer.2.output_layer_norm.weight: False\n",
      "distilbert.transformer.layer.2.output_layer_norm.bias: False\n",
      "distilbert.transformer.layer.3.attention.q_lin.linear.weight: False\n",
      "distilbert.transformer.layer.3.attention.q_lin.linear.bias: False\n",
      "distilbert.transformer.layer.3.attention.q_lin.lora.W_a: True\n",
      "distilbert.transformer.layer.3.attention.q_lin.lora.W_b: True\n",
      "distilbert.transformer.layer.3.attention.k_lin.weight: False\n",
      "distilbert.transformer.layer.3.attention.k_lin.bias: False\n",
      "distilbert.transformer.layer.3.attention.v_lin.linear.weight: False\n",
      "distilbert.transformer.layer.3.attention.v_lin.linear.bias: False\n",
      "distilbert.transformer.layer.3.attention.v_lin.lora.W_a: True\n",
      "distilbert.transformer.layer.3.attention.v_lin.lora.W_b: True\n",
      "distilbert.transformer.layer.3.attention.out_lin.weight: False\n",
      "distilbert.transformer.layer.3.attention.out_lin.bias: False\n",
      "distilbert.transformer.layer.3.sa_layer_norm.weight: False\n",
      "distilbert.transformer.layer.3.sa_layer_norm.bias: False\n",
      "distilbert.transformer.layer.3.ffn.lin1.weight: False\n",
      "distilbert.transformer.layer.3.ffn.lin1.bias: False\n",
      "distilbert.transformer.layer.3.ffn.lin2.weight: False\n",
      "distilbert.transformer.layer.3.ffn.lin2.bias: False\n",
      "distilbert.transformer.layer.3.output_layer_norm.weight: False\n",
      "distilbert.transformer.layer.3.output_layer_norm.bias: False\n",
      "distilbert.transformer.layer.4.attention.q_lin.linear.weight: False\n",
      "distilbert.transformer.layer.4.attention.q_lin.linear.bias: False\n",
      "distilbert.transformer.layer.4.attention.q_lin.lora.W_a: True\n",
      "distilbert.transformer.layer.4.attention.q_lin.lora.W_b: True\n",
      "distilbert.transformer.layer.4.attention.k_lin.weight: False\n",
      "distilbert.transformer.layer.4.attention.k_lin.bias: False\n",
      "distilbert.transformer.layer.4.attention.v_lin.linear.weight: False\n",
      "distilbert.transformer.layer.4.attention.v_lin.linear.bias: False\n",
      "distilbert.transformer.layer.4.attention.v_lin.lora.W_a: True\n",
      "distilbert.transformer.layer.4.attention.v_lin.lora.W_b: True\n",
      "distilbert.transformer.layer.4.attention.out_lin.weight: False\n",
      "distilbert.transformer.layer.4.attention.out_lin.bias: False\n",
      "distilbert.transformer.layer.4.sa_layer_norm.weight: False\n",
      "distilbert.transformer.layer.4.sa_layer_norm.bias: False\n",
      "distilbert.transformer.layer.4.ffn.lin1.weight: False\n",
      "distilbert.transformer.layer.4.ffn.lin1.bias: False\n",
      "distilbert.transformer.layer.4.ffn.lin2.weight: False\n",
      "distilbert.transformer.layer.4.ffn.lin2.bias: False\n",
      "distilbert.transformer.layer.4.output_layer_norm.weight: False\n",
      "distilbert.transformer.layer.4.output_layer_norm.bias: False\n",
      "distilbert.transformer.layer.5.attention.q_lin.linear.weight: False\n",
      "distilbert.transformer.layer.5.attention.q_lin.linear.bias: False\n",
      "distilbert.transformer.layer.5.attention.q_lin.lora.W_a: True\n",
      "distilbert.transformer.layer.5.attention.q_lin.lora.W_b: True\n",
      "distilbert.transformer.layer.5.attention.k_lin.weight: False\n",
      "distilbert.transformer.layer.5.attention.k_lin.bias: False\n",
      "distilbert.transformer.layer.5.attention.v_lin.linear.weight: False\n",
      "distilbert.transformer.layer.5.attention.v_lin.linear.bias: False\n",
      "distilbert.transformer.layer.5.attention.v_lin.lora.W_a: True\n",
      "distilbert.transformer.layer.5.attention.v_lin.lora.W_b: True\n",
      "distilbert.transformer.layer.5.attention.out_lin.weight: False\n",
      "distilbert.transformer.layer.5.attention.out_lin.bias: False\n",
      "distilbert.transformer.layer.5.sa_layer_norm.weight: False\n",
      "distilbert.transformer.layer.5.sa_layer_norm.bias: False\n",
      "distilbert.transformer.layer.5.ffn.lin1.weight: False\n",
      "distilbert.transformer.layer.5.ffn.lin1.bias: False\n",
      "distilbert.transformer.layer.5.ffn.lin2.weight: False\n",
      "distilbert.transformer.layer.5.ffn.lin2.bias: False\n",
      "distilbert.transformer.layer.5.output_layer_norm.weight: False\n",
      "distilbert.transformer.layer.5.output_layer_norm.bias: False\n",
      "pre_classifier.weight: False\n",
      "pre_classifier.bias: False\n",
      "classifier.weight: False\n",
      "classifier.bias: False\n"
     ]
    }
   ],
   "source": [
    "# Check if linear layers are frozen\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc77229-2ab1-4d9a-b7a6-d37610e00ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters: 147456\n"
     ]
    }
   ],
   "source": [
    "# Utility to count trainable parameters\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(\"Total number of trainable parameters:\", count_parameters(model))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83971bc4-143d-4d82-b5cb-8774f9112f53",
   "metadata": {},
   "source": [
    "# 5 Finetuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "730b9a87-709e-4f3d-9c9f-475ccbb56908",
   "metadata": {},
   "source": [
    "**Wrap in LightningModule for Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2c474d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap model in custom PyTorch Lightning module\n",
    "from local_model_utilities import CustomLightningModule\n",
    "\n",
    "lightning_model = CustomLightningModule(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dab813-e1fc-47cd-87a1-5eb8070699c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks: save best model by validation accuracy\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        save_top_k=1, mode=\"max\", monitor=\"val_acc\"\n",
    "    )  # save top 1 model\n",
    "]\n",
    "logger = CSVLogger(save_dir=\"logs/\", name=\"distilbert_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492aa043-02da-459e-a266-091b34254ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# Setup Lightning Trainer\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=3,\n",
    "    callbacks=callbacks,\n",
    "    accelerator=\"gpu\",\n",
    "    precision=\"16-mixed\",\n",
    "    devices=1,\n",
    "    logger=logger,\n",
    "    log_every_n_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff08ea9a-faf6-410d-9a2f-a0d2b92dd027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model and track elapsed time\n",
    "\n",
    "# import time\n",
    "# start = time.time()\n",
    "\n",
    "# trainer.fit(model=lightning_model,\n",
    "#             train_dataloaders=train_loader,\n",
    "#             val_dataloaders=val_loader)\n",
    "\n",
    "# end = time.time()\n",
    "# elapsed = end - start\n",
    "# print(f\"Time elapsed {elapsed/60:.2f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe33b8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best saved model checkpoint\n",
    "\n",
    "saved_model_path=\"logs/distilbert_lora/version_0/checkpoints/epoch=0-step=2917.ckpt\" # trained in the full-data\n",
    "# train_model_path=\"logs/distilbert_lora/version_1/checkpoints/epoch=0-step=2.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d795778a-70d2-4b04-96fb-598eccbcd1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA H100 PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Restoring states from the checkpoint path at logs/distilbert_lora/version_0/checkpoints/epoch=0-step=2917.ckpt\n",
      "/home/a14-hliu/miniconda3/envs/py311/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:362: The dirpath has changed from 'logs/my-model/version_1/checkpoints' to 'logs/distilbert_lora/version_2/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Loaded model weights from the checkpoint at logs/distilbert_lora/version_0/checkpoints/epoch=0-step=2917.ckpt\n",
      "/home/a14-hliu/miniconda3/envs/py311/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  3.24it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at logs/distilbert_lora/version_0/checkpoints/epoch=0-step=2917.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Loaded model weights from the checkpoint at logs/distilbert_lora/version_0/checkpoints/epoch=0-step=2917.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 79.59it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at logs/distilbert_lora/version_0/checkpoints/epoch=0-step=2917.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Loaded model weights from the checkpoint at logs/distilbert_lora/version_0/checkpoints/epoch=0-step=2917.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 65.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on each dataset split\n",
    "\n",
    "train_acc = trainer.test(lightning_model, dataloaders=train_loader, ckpt_path=saved_model_path, verbose=False)\n",
    "val_acc = trainer.test(lightning_model, dataloaders=val_loader, ckpt_path=saved_model_path, verbose=False)\n",
    "test_acc = trainer.test(lightning_model, dataloaders=test_loader, ckpt_path=saved_model_path, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f211076-cfea-4779-a05c-eb8073ee41dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 100.00%\n",
      "Val acc:   85.71%\n",
      "Test acc:  80.00%\n"
     ]
    }
   ],
   "source": [
    "# Print final evaluation results\n",
    "\n",
    "print(f\"Train acc: {train_acc[0]['accuracy']*100:2.2f}%\")\n",
    "print(f\"Val acc:   {val_acc[0]['accuracy']*100:2.2f}%\")\n",
    "print(f\"Test acc:  {test_acc[0]['accuracy']*100:2.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3855d32-906b-46b9-b536-61d6cbefcb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional cleanup of model checkpoints and logs\n",
    "\n",
    "# import shutil\n",
    "\n",
    "# # Cleanup checkpoint files as we don't need them later\n",
    "# log_dir = f\"logs/my-model\"\n",
    "# if os.path.exists(log_dir):\n",
    "#     shutil.rmtree(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fee576a-2cc5-4bc6-befc-eeac97723e0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
